{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133c44d3",
   "metadata": {},
   "source": [
    "## 1. Single image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2be7b0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed set to: 0\n",
      "--- Loading Single-Frame Datasets ---\n",
      "  Train Class Counts: [1885 2560 2917]\n",
      "Starting Training on 7362 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 148/148 [00:13<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 0.7738 | Val Acc: 0.8414\n",
      "  >>> New Best Model Saved! (Acc: 0.8414)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 148/148 [00:13<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 0.8898 | Val Acc: 0.8738\n",
      "  >>> New Best Model Saved! (Acc: 0.8738)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 148/148 [00:13<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 0.9390 | Val Acc: 0.8770\n",
      "  >>> New Best Model Saved! (Acc: 0.8770)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 148/148 [00:13<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 0.9666 | Val Acc: 0.8835\n",
      "  >>> New Best Model Saved! (Acc: 0.8835)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 148/148 [00:13<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 0.9863 | Val Acc: 0.8867\n",
      "  >>> New Best Model Saved! (Acc: 0.8867)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 148/148 [00:13<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc: 0.9928 | Val Acc: 0.8770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 148/148 [00:13<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc: 0.9973 | Val Acc: 0.8867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 148/148 [00:13<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc: 0.9978 | Val Acc: 0.8803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 148/148 [00:13<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc: 0.9992 | Val Acc: 0.8900\n",
      "  >>> New Best Model Saved! (Acc: 0.8900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 148/148 [00:13<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc: 0.9990 | Val Acc: 0.8835\n",
      "Training Complete. Best Validation Accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_DIR = \"/home/dad/Desktop/temporal_test_images/data/single_image\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "\n",
    "BATCH_SIZE    = 65\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS    = 10\n",
    "NUM_CLASSES   = 3 \n",
    "SEED          = 0  # Fixed Seed\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Fix all random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random Seed set to: {seed}\")\n",
    "\n",
    "def calculate_weights(dataset):\n",
    "    targets = dataset.targets\n",
    "    counts = np.bincount(targets)\n",
    "    print(f\"  Train Class Counts: {counts}\")\n",
    "    counts = np.maximum(counts, 1) \n",
    "    weights = len(targets) / (len(counts) * counts)\n",
    "    return torch.FloatTensor(weights).to(device)\n",
    "\n",
    "def train_single_frame():\n",
    "    set_seed(SEED) # <--- APPLY SEED HERE\n",
    "\n",
    "    # Standard ResNet Transforms\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"--- Loading Single-Frame Datasets ---\")\n",
    "    train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=data_transforms)\n",
    "    val_dataset = datasets.ImageFolder(VAL_DIR, transform=data_transforms)\n",
    "    \n",
    "    # Dataloaders (Worker seeding is handled by PyTorch usually, but main seed is key)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    class_weights = calculate_weights(train_dataset)\n",
    "\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "    print(f\"Starting Training on {len(train_dataset)} images...\")\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), \"best_single_frame_resnet18.pth\")\n",
    "            print(f\"  >>> New Best Model Saved! (Acc: {best_acc:.4f})\")\n",
    "\n",
    "    print(f\"Training Complete. Best Validation Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_single_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f681d2f",
   "metadata": {},
   "source": [
    "## 2. Early fusion classification (3 frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa06121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed set to: 0\n",
      "--- Loading Temporal Datasets ---\n",
      "  Train Class Counts: [2917 1885 2560]\n",
      "Starting Training on 7362 triplets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 114/114 [00:18<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 0.7460 | Val Acc: 0.8479\n",
      "  >>> New Best Model Saved! (Acc: 0.8479)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 114/114 [00:18<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 0.8890 | Val Acc: 0.9191\n",
      "  >>> New Best Model Saved! (Acc: 0.9191)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 114/114 [00:18<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 0.9321 | Val Acc: 0.9029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 114/114 [00:18<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc: 0.9612 | Val Acc: 0.9061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 114/114 [00:18<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc: 0.9766 | Val Acc: 0.8997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 114/114 [00:18<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc: 0.9908 | Val Acc: 0.9159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 114/114 [00:18<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc: 0.9954 | Val Acc: 0.9029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 114/114 [00:18<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc: 0.9974 | Val Acc: 0.9191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 114/114 [00:18<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc: 0.9977 | Val Acc: 0.9256\n",
      "  >>> New Best Model Saved! (Acc: 0.9256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 114/114 [00:18<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc: 0.9993 | Val Acc: 0.9256\n",
      "Training Complete. Best Validation Accuracy: 0.9256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_DIR = \"/home/dad/Desktop/temporal_test_images/data/temporal_images\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "\n",
    "BATCH_SIZE = 65\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "NUM_CLASSES = 3 \n",
    "SEED = 0# Fixed Seed\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Fix all random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random Seed set to: {seed}\")\n",
    "\n",
    "class TemporalStackedDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = [] \n",
    "        self.class_to_idx = {'U': 0, 'D': 1, 'P': 2} \n",
    "        \n",
    "        for class_name, class_idx in self.class_to_idx.items():\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_path): continue\n",
    "            \n",
    "            t0_files = glob.glob(os.path.join(class_path, \"*_t0.jpg\"))\n",
    "            for t0_path in t0_files:\n",
    "                t1_path = t0_path.replace(\"_t0.jpg\", \"_t1.jpg\")\n",
    "                t2_path = t0_path.replace(\"_t0.jpg\", \"_t2.jpg\")\n",
    "                \n",
    "                if os.path.exists(t1_path) and os.path.exists(t2_path):\n",
    "                    self.samples.append((t0_path, t1_path, t2_path, class_idx))\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_t0, path_t1, path_t2, label = self.samples[idx]\n",
    "        img_t0 = Image.open(path_t0).convert('RGB')\n",
    "        img_t1 = Image.open(path_t1).convert('RGB')\n",
    "        img_t2 = Image.open(path_t2).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img_t0 = self.transform(img_t0)\n",
    "            img_t1 = self.transform(img_t1)\n",
    "            img_t2 = self.transform(img_t2)\n",
    "\n",
    "        stacked_imgs = torch.cat([img_t0, img_t1, img_t2], dim=0)\n",
    "        return stacked_imgs, label\n",
    "\n",
    "def get_temporal_model(num_classes=3):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    original_weights = model.conv1.weight.data\n",
    "    new_conv1 = nn.Conv2d(9, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    new_weights = torch.cat([original_weights, original_weights, original_weights], dim=1)\n",
    "    new_conv1.weight.data = new_weights / 3.0\n",
    "    model.conv1 = new_conv1\n",
    "    \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "def calculate_weights(dataset):\n",
    "    counts = np.bincount(dataset.labels)\n",
    "    print(f\"  Train Class Counts: {counts}\")\n",
    "    counts = np.maximum(counts, 1)\n",
    "    weights = len(dataset.labels) / (len(counts) * counts)\n",
    "    return torch.FloatTensor(weights).to(device)\n",
    "\n",
    "def train_temporal():\n",
    "    set_seed(SEED) # <--- APPLY SEED HERE\n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    ])\n",
    "\n",
    "    print(\"--- Loading Temporal Datasets ---\")\n",
    "    train_dataset = TemporalStackedDataset(TRAIN_DIR, transform=data_transforms)\n",
    "    val_dataset = TemporalStackedDataset(VAL_DIR, transform=data_transforms)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    class_weights = calculate_weights(train_dataset)\n",
    "\n",
    "    model = get_temporal_model(NUM_CLASSES)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "    print(f\"Starting Training on {len(train_dataset)} triplets...\")\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), \"best_temporal_resnet18_9ch.pth\")\n",
    "            print(f\"  >>> New Best Model Saved! (Acc: {best_acc:.4f})\")\n",
    "\n",
    "    print(f\"Training Complete. Best Validation Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_temporal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd8bafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dad/Desktop/temporal_test_images'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4cd562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dad/anaconda3/envs/hj/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dad/anaconda3/envs/hj/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Single-Frame Model from best_single_frame_resnet18.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Single-Frame: 100%|██████████| 10/10 [00:00<00:00, 21.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Single-Frame Overall Accuracy: 88.37%\n",
      "    Up (U) Accuracy: 82.00%\n",
      "    Down (D) Accuracy: 94.06%\n",
      "    Pass (P) Accuracy: 89.00%\n",
      "Loading Temporal Model from best_temporal_resnet18_9ch.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Temporal: 100%|██████████| 10/10 [00:01<00:00,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Temporal Overall Accuracy: 89.37%\n",
      "    Up (U) Accuracy: 83.00%\n",
      "    Down (D) Accuracy: 90.00%\n",
      "    Pass (P) Accuracy: 95.05%\n",
      "\n",
      "========================================\n",
      "FINAL TEST SET RESULTS\n",
      "Single-Frame Model: 88.37%\n",
      "Temporal Model:     89.37%\n",
      "Improvement:        +1.00%\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# 1. Path to your Test Sets\n",
    "SINGLE_TEST_DIR = \"/home/dad/Desktop/temporal_test_images/data/single_image/test\"\n",
    "TEMPORAL_TEST_DIR = \"/home/dad/Desktop/temporal_test_images/data/temporal_images/test\"\n",
    "\n",
    "# 2. Model Files (Must allow match the names you saved)\n",
    "SINGLE_MODEL_PATH = \"best_single_frame_resnet18.pth\"\n",
    "TEMPORAL_MODEL_PATH = \"best_temporal_resnet18_9ch.pth\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- DATASET CLASSES (Must match training code) ---\n",
    "class TemporalStackedDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        # Ensure exact same class mapping as training\n",
    "        self.class_to_idx = {'U': 0, 'D': 1, 'P': 2}\n",
    "        \n",
    "        for class_name, class_idx in self.class_to_idx.items():\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_path): continue\n",
    "            \n",
    "            t0_files = glob.glob(os.path.join(class_path, \"*_t0.jpg\"))\n",
    "            for t0_path in t0_files:\n",
    "                t1_path = t0_path.replace(\"_t0.jpg\", \"_t1.jpg\")\n",
    "                t2_path = t0_path.replace(\"_t0.jpg\", \"_t2.jpg\")\n",
    "                if os.path.exists(t1_path) and os.path.exists(t2_path):\n",
    "                    self.samples.append((t0_path, t1_path, t2_path, class_idx))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_t0, path_t1, path_t2, label = self.samples[idx]\n",
    "        img_t0 = Image.open(path_t0).convert('RGB')\n",
    "        img_t1 = Image.open(path_t1).convert('RGB')\n",
    "        img_t2 = Image.open(path_t2).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img_t0 = self.transform(img_t0)\n",
    "            img_t1 = self.transform(img_t1)\n",
    "            img_t2 = self.transform(img_t2)\n",
    "            \n",
    "        return torch.cat([img_t0, img_t1, img_t2], dim=0), label\n",
    "\n",
    "# --- MODEL LOADERS ---\n",
    "def load_single_model(path):\n",
    "    print(f\"Loading Single-Frame Model from {path}...\")\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_temporal_model(path):\n",
    "    print(f\"Loading Temporal Model from {path}...\")\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    # Rebuild 9-channel layer structure\n",
    "    new_conv1 = nn.Conv2d(9, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.conv1 = new_conv1\n",
    "    \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "    \n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# --- EVALUATION FUNCTION ---\n",
    "def evaluate(model, dataloader, name):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Optional: Track per-class accuracy\n",
    "    class_correct = list(0. for i in range(NUM_CLASSES))\n",
    "    class_total = list(0. for i in range(NUM_CLASSES))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Testing {name}\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Per class calculation\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print(f\"\\n>>> {name} Overall Accuracy: {acc:.2f}%\")\n",
    "    \n",
    "    classes = ['Up (U)', 'Down (D)', 'Pass (P)']\n",
    "    for i in range(NUM_CLASSES):\n",
    "        if class_total[i] > 0:\n",
    "            print(f\"    {classes[i]} Accuracy: {100 * class_correct[i] / class_total[i]:.2f}%\")\n",
    "            \n",
    "    return acc\n",
    "\n",
    "# --- MAIN ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Standard Transforms (Same as training)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 1. Evaluate Single Frame\n",
    "    if os.path.exists(SINGLE_MODEL_PATH):\n",
    "        single_dataset = datasets.ImageFolder(SINGLE_TEST_DIR, transform=transform)\n",
    "        single_loader = DataLoader(single_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        model_single = load_single_model(SINGLE_MODEL_PATH)\n",
    "        acc_single = evaluate(model_single, single_loader, \"Single-Frame\")\n",
    "    else:\n",
    "        print(f\"Error: Could not find {SINGLE_MODEL_PATH}\")\n",
    "        acc_single = 0\n",
    "\n",
    "    # 2. Evaluate Temporal\n",
    "    if os.path.exists(TEMPORAL_MODEL_PATH):\n",
    "        temporal_dataset = TemporalStackedDataset(TEMPORAL_TEST_DIR, transform=transform)\n",
    "        temporal_loader = DataLoader(temporal_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        model_temporal = load_temporal_model(TEMPORAL_MODEL_PATH)\n",
    "        acc_temporal = evaluate(model_temporal, temporal_loader, \"Temporal\")\n",
    "    else:\n",
    "        print(f\"Error: Could not find {TEMPORAL_MODEL_PATH}\")\n",
    "        acc_temporal = 0\n",
    "\n",
    "    # 3. Final Report\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"FINAL TEST SET RESULTS\")\n",
    "    print(f\"Single-Frame Model: {acc_single:.2f}%\")\n",
    "    print(f\"Temporal Model:     {acc_temporal:.2f}%\")\n",
    "    print(f\"Improvement:        {acc_temporal - acc_single:+.2f}%\")\n",
    "    print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
